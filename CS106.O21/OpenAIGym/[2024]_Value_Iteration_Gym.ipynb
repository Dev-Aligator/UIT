{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YczE55EXiYx9",
        "outputId": "4fd75f13-3eae-4fb5-cff8-7b4929fcf155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_vsP04EnmIU"
      },
      "source": [
        "# FROZEN LAKE V1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "outputs": [],
      "source": [
        "env1 = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "5619464e-0146-4380-c54d-2364a1125dfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "env1.P[0][3] # Transition model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "e65f3a76-4e9a-40e8-a716-de61e1eebf53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "env1.observation_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "c415ca8c-915b-44ff-f43a-e42ad4bf1b01"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "env1.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "outputs": [],
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "outputs": [],
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "outputs": [],
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YOQ7Hs4DqX2T"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(env, v_values, gamma = 0.9):\n",
        "  \"\"\"\n",
        "  Performs policy improvement based on the given value function.\n",
        "\n",
        "  Args:\n",
        "    env (gym.Env): The OpenAI Gym environment.\n",
        "    v_values (numpy.ndarray): The value function for each state.\n",
        "    gamma (float, optional): The discount factor. Defaults to 0.9.\n",
        "\n",
        "  Returns:\n",
        "    numpy.ndarray: The improved policy.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the policy with zeros\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "  # Iterate over each state\n",
        "  for state in range(env.observation_space.n):\n",
        "\n",
        "    # Initialize the Q-values for each action\n",
        "    q_values = np.zeros(env.action_space.n)\n",
        "\n",
        "    # Iterate over each action\n",
        "    for action in range(env.action_space.n):\n",
        "\n",
        "      # Iterate over each possible transition from the current state and action\n",
        "      for prob, next_state, reward, done in env.P[state][action]:\n",
        "\n",
        "        # Calculate the Q-value for the current state-action pair\n",
        "        q_values[action] += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "    # Update the policy with the action that maximizes the Q-value\n",
        "    policy[state] = np.argmax(q_values)\n",
        "\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "u7H4k8i5tYqV"
      },
      "outputs": [],
      "source": [
        "# The function policy_iteration is defined with three parameters: env (the environment), max_iters (the maximum number of iterations), and gamma (the discount factor).\n",
        "def policy_iteration(env, max_iters = 1000, gamma = 0.9):\n",
        "\n",
        "  # Initialize the policy with a zero array of the same size as the observation space in the environment.\n",
        "  policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "  # Start a loop that will run for a maximum of max_iters iterations.\n",
        "  for i in range(max_iters):\n",
        "\n",
        "    # Evaluate the current policy to get the value function.\n",
        "    value_function = policy_evaluation(env,policy)\n",
        "\n",
        "    # Extract a new policy based on the current value function.\n",
        "    new_policy = policy_extraction(env,value_function)\n",
        "\n",
        "    # If the new policy is the same as the old policy (i.e., the policy has converged), break the loop.\n",
        "    if (np.all(policy == new_policy)):\n",
        "      break\n",
        "\n",
        "    # If the policy has not converged, update the policy with the new policy and continue the loop.\n",
        "    policy = new_policy\n",
        "\n",
        "  # Return the final policy after the loop has finished.\n",
        "  return policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge5DYtfXofop",
        "outputId": "fe518ef2-9627-4302-e361-2eb5a9b7f599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n",
            "Number of successes: 775/1000\n",
            "Average number of steps: 43.681290322580644\n",
            "Value Iteration Running Time: 1.004274606704712\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "optimal_vvalue1 = value_iteration(env1)\n",
        "optimal_policy1 = policy_extraction(env1,optimal_vvalue1)\n",
        "play_multiple_times(env1,optimal_policy1,1000)\n",
        "end_time = time.time()\n",
        "print(\"Value Iteration Running Time:\",end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXLd8KMfo_I6",
        "outputId": "23ab312e-36b4-4b51-ea83-39e642b7ba37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 0-th iteration.\n",
            "Converged at 23-th iteration.\n",
            "Converged at 59-th iteration.\n",
            "Converged at 62-th iteration.\n",
            "Converged at 79-th iteration.\n",
            "Converged at 80-th iteration.\n",
            "Number of successes: 778/1000\n",
            "Average number of steps: 43.03341902313625\n",
            "Policy Iteration Running Time: 0.9025859832763672\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "optimal_policy1 = policy_iteration(env1)\n",
        "play_multiple_times(env1,optimal_policy1,1000)\n",
        "end_time = time.time()\n",
        "print(\"Policy Iteration Running Time:\",end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7cNEFCvpEOt",
        "outputId": "295d27fb-4b79-4071-f493-2695321d1da4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "optimal_policy1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka-RUzVWpI_x"
      },
      "source": [
        "# FrozenLake8x8-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ILNp9icmpNuD"
      },
      "outputs": [],
      "source": [
        "env2 = gym.make('FrozenLake8x8-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY_zcQ_UpZeX",
        "outputId": "88fa3289-a75e-4024-f5ee-484b5cc35b2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "env2.observation_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0PipWMJpbTv",
        "outputId": "adb35274-dd67-45d4-804a-bd6ae4fd7bcd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "env2.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ4DncDFpdW8",
        "outputId": "358426bb-336e-4cb5-e418-426ba7dc2c84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 117-th iteration.\n",
            "Number of successes: 720/1000\n",
            "Average number of steps: 72.60277777777777\n",
            "Value Iteration Running Time: 2.11513352394104\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "optimal_vvalue2 = value_iteration(env2)\n",
        "optimal_policy2 = policy_extraction(env2,optimal_vvalue2)\n",
        "play_multiple_times(env2,optimal_policy2,1000)\n",
        "end_time = time.time()\n",
        "print(\"Value Iteration Running Time:\",end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nz4636uLpjV3",
        "outputId": "60ea3c76-fc58-4967-bb99-7c6875c03318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 27-th iteration.\n",
            "Converged at 91-th iteration.\n",
            "Converged at 92-th iteration.\n",
            "Converged at 86-th iteration.\n",
            "Converged at 90-th iteration.\n",
            "Converged at 92-th iteration.\n",
            "Converged at 95-th iteration.\n",
            "Converged at 100-th iteration.\n",
            "Converged at 112-th iteration.\n",
            "Converged at 117-th iteration.\n",
            "Number of successes: 740/1000\n",
            "Average number of steps: 74.40810810810811\n",
            "Policy Iteration Running Time: 2.208096504211426\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "optimal_policy2 = policy_iteration(env2)\n",
        "play_multiple_times(env2,optimal_policy2,1000)\n",
        "end_time = time.time()\n",
        "print(\"Policy Iteration Running Time:\",end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEHICQklppta",
        "outputId": "7ee7a84c-eb57-4f99-fbc7-979d94ce40ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 1, 3, 3, 0, 0, 2, 3,\n",
              "       2, 1, 3, 3, 3, 1, 0, 0, 2, 1, 3, 3, 0, 0, 2, 1, 3, 2, 0, 0, 0, 1,\n",
              "       3, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "optimal_policy2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1p51E7CXpq43"
      },
      "source": [
        "# Taxi-v3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FDC06Ix0pszu"
      },
      "outputs": [],
      "source": [
        "env3 = gym.make('Taxi-v3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy4w5nswpveX",
        "outputId": "be63a7db-18c9-4fd2-d588-2b8ba2291f89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "env3.observation_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2kQietypwxO",
        "outputId": "f5887968-add5-4928-f3d3-18079f445117"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "env3.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeGfHuznpyew",
        "outputId": "f34747e2-ac74-4dcd-851d-01b8379dcbae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 116-th iteration.\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.301\n",
            "Value Iteration Running Time: 7.952168703079224\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "optimal_vvalue3 = value_iteration(env3)\n",
        "optimal_policy3 = policy_extraction(env3,optimal_vvalue3)\n",
        "play_multiple_times(env3,optimal_policy3,1000)\n",
        "end_time = time.time()\n",
        "print(\"Value Iteration Running Time:\",end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGiMR0L-p4bj",
        "outputId": "93bc9077-5f7f-4429-b7aa-506e1fcfa82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 88-th iteration.\n",
            "Converged at 97-th iteration.\n",
            "Converged at 100-th iteration.\n",
            "Converged at 101-th iteration.\n",
            "Converged at 102-th iteration.\n",
            "Converged at 103-th iteration.\n",
            "Converged at 106-th iteration.\n",
            "Converged at 109-th iteration.\n",
            "Converged at 110-th iteration.\n",
            "Converged at 111-th iteration.\n",
            "Converged at 112-th iteration.\n",
            "Converged at 115-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Converged at 116-th iteration.\n",
            "Number of successes: 1000/1000\n",
            "Average number of steps: 13.007\n",
            "Policy Iteration Running Time: 12.71245265007019\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "optimal_policy3 = policy_iteration(env3)\n",
        "play_multiple_times(env3,optimal_policy3,1000)\n",
        "end_time = time.time()\n",
        "print(\"Policy Iteration Running Time:\",end_time - start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IafSM484qE-k",
        "outputId": "c3079c3e-147a-4e6c-c8d1-4370e11f1ddb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 3, 3,\n",
              "       3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0,\n",
              "       2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2,\n",
              "       2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "       0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 1, 2, 0, 2,\n",
              "       1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 1, 2, 3, 2, 3, 3,\n",
              "       3, 3, 1, 1, 1, 1, 3, 3, 3, 3, 2, 2, 2, 2, 3, 1, 3, 2, 3, 3, 3, 3,\n",
              "       1, 1, 1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 3, 1, 3, 0, 3, 3, 3, 3, 1, 1,\n",
              "       1, 1, 3, 3, 3, 3, 0, 0, 0, 0, 3, 1, 3, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
              "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
              "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1,\n",
              "       5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 1, 1, 1, 5, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 1, 1, 1, 3], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "optimal_policy3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2R6Awz6qOGJ"
      },
      "source": [
        "# Nhận xét và kết quả"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKd6qTmQqe-Z"
      },
      "source": [
        "- Policy Iteration và Value Iteration đều cho ra tỉ lệ thành công tương đương nhau (ở Taxi-V3 tỉ lệ thành công là 100%).\n",
        "- Khi so về thời gian chạy, Value Iteration chạy nhanh hơn Policy Iteration một chút. Đặc biệt ở map 'Taxi-v3', khi độ chênh lệch ở đây là khoảng 5 giây.\n",
        "- Khi so về trung bình số bước đi, sự chênh lệch của 2 thuật toán là không đánh kể, chỉ chênh nhau một vài bước.\n",
        "- Nhìn chung, với kết quả và tỉ lệ thành công tương tự nhau sau 1000 lần chạy, Value Iteration vượt Policy Iteration về mặt thời gian. Tuy nhiên sự chênh lệch chỉ thực sự đáng kể khi được thử nghiệm ở map 'Taxi v3'."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}